fix_seed: 0
checkpoints_every: 256
tensorboard_log: True
tqdm_progress_bar: False
result_path: ./results/test/

resume:
  enabled: False
  resume_path: results/dgx/vqvae/2025-07-02__01-28-28/checkpoints/epoch_8.pth
  restart_optimizer: True
  discard_decoder_weights: True

model:
  compile_model: False
  max_length: 512
  decoder_output_scaling_factor: 1  # Added scaling factor for backbone prediction outputs
  use_ndlinear: False  # Toggle for using NdLinear instead of Conv1d layers
  encoder:
    name: gcpnet # gvp_transformer, gcpnet
    freeze_parameters: False
    pretrained:
      enabled: True
      config_path: ./configs/pretrained/structure_denoising_pretrained_config.yaml
      checkpoint_path: ./models/checkpoints/structure_denoising/gcpnet/ca_bb/last.ckpt # define your checkpoint directory here
  vqvae:
    vector_quantization:
      enabled: True
      freeze_parameters: False
      dim: 256
      decay: 0.99
      codebook_size: 4096
      commitment_weight: 0.5
      orthogonal_reg_weight: 10
      orthogonal_reg_max_codes: 512
      orthogonal_reg_active_codes_only: True
      rotation_trick: True
      threshold_ema_dead_code: 2
      kmeans_init: True
      kmeans_iters: 10
      alpha: 1.0
    encoder:
      freeze_parameters: False
      dimension: 1024  # Used as dim_in and dim_out for ContinuousTransformerWrapper
      ff_mult: 4         # Multiplier for the feedforward dimension
      depth: 12      # Number of layers in the Encoder
      heads: 12      # Number of attention heads in the Encoder
      rotary_pos_emb: True
      attn_flash: True # FA-2 if installed
      attn_kv_heads: 3 # GQA
      qk_norm: True
      pre_norm: True
      residual_attn: False # Set pre_norm to False if residual_attn is True
      num_memory_tokens: 0 # Number of memory tokens, 0 means no memory tokens
    decoder:
      name: geometric_decoder # geometric_decoder, gcpnet
      freeze_parameters: False

train_settings:
  data_path: ../../datasets/vqvae/whole_validation_2048_h5/
  num_epochs: 256
  shuffle: True
  loss: crossentropy
  sample_weight: False
  mixed_precision: bf16 # no, fp16, bf16, fp8
  save_pdb_every: 8
  batch_size: 16
  num_workers: 8
  grad_accumulation: 1
  max_task_samples: 2000
  profile_train_loop: False
  cutoff_augmentation:
    enabled: False
    probability: 0.5
    min_length: 25
  nan_augmentation:
    enabled: True
    probability: 0.05
    max_length: 7
  gradient_norm_logging_freq: 50  # How often to calculate and log gradient norm (in steps)
  losses:
    alignment_strategy: kabsch # kabsch, kabsch_old, quaternion, no
    mse:
      enabled: True
      weight: 0.005
    backbone_distance:
      enabled: True
      weight: 0.01
    backbone_direction:
      enabled: True
      weight: 0.05
    binned_distance_classification:
      enabled: False
      weight: 0.01
    binned_direction_classification:
      enabled: False
      weight: 0.01
    fape:
      enabled: False
      weight: 1.0
      clamp_distance: 10.0
      length_scale: 10.0

valid_settings:
  data_path: ../../datasets/vqvae/whole_validation_2048_h5
  do_every: 1
  save_pdb_every: 1
  batch_size: 32
  num_workers: 0

visualization_settings:
  data_path: ../../datasets/vqvae/whole_validation_2048_h5
  fasta_path: visualization/Rep_subfamily_basedon_S40pdb.fa
  do_every: 8192
  batch_size: 1
  num_workers: 4

optimizer:
  name: adam
  lr: 1e-4
  weight_decouple: True
  weight_decay: 1e-3
  eps: 1e-7
  beta_1: 0.9
  beta_2: 0.98
  use_8bit_adam: False
  grad_clip_norm: 1
  decay:
    warmup: 2048
    min_lr: 1e-6
    gamma: 0.2
    num_restarts: 1
