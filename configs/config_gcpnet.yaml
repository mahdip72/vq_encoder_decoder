fix_seed: 0
checkpoints_every: 1024
tensorboard_log: True
tqdm_progress_bar: False
result_path: ./results/gcpnet_vqvae/test
normalizer_path: ./data/normalizer.pkl

resume:
  resume: False
  resume_path: ./path/to/checkpoint.pth
  restart_optimizer: True

model:
  architecture: gcpnet_vqvae
  compile_model: False
  max_length: 128
  struct_encoder:
    enable: False
    use_rotary_embeddings: False
    use_positional_embeddings: True
    use_foldseek: False
    use_foldseek_vector: False
    top_k: 30 #default 30 The number of edges to draw per node (as destination node).
    num_positional_embeddings: 16  #default 16
    use_seq:
       enable: False
       seq_embed_mode: "embedding" #"embedding","ESM2"
       seq_embed_dim: 20 #1280 for ESM2
    fine_tune:
      enable: False
      last_layers_trainable: 5

    emb_dim: 128
    node_s_emb_dim: ${.emb_dim} # Dimension of the node state embeddings
    node_v_emb_dim: 16 # Dimension of the node vector embeddings
    edge_s_emb_dim: 32 # Dimension of the edge state embeddings
    edge_v_emb_dim: 4 # Dimension of the edge vector embeddings

    # GCPNet module config #
    module_cfg:
      norm_pos_diff: true
      scalar_gate: 0
      vector_gate: true
      scalar_nonlinearity: silu
      vector_nonlinearity: silu
      nonlinearities:
        - silu
        - silu
      r_max: 10.0
      num_rbf: 8
      bottleneck: 4
      vector_linear: true
      vector_identity: true
      default_bottleneck: 4
      predict_backbone_positions: false  # note: if `false`, then the input node backbone positions will not be updated
      predict_node_rep: true  # note: if `false`, then a final projection of the node features will not be performed
      node_positions_weight: 1.0
      update_positions_with_vector_sum: false
      enable_e3_equivariance: false
      pool: sum
    # GCPNet model config #
    model_cfg:
      h_input_dim: 6
      chi_input_dim: 3
      e_input_dim: 8
      xi_input_dim: 1
      # note: each `hidden_dim` must be evenly divisible by `bottleneck`
      h_hidden_dim: 128
      chi_hidden_dim: 16
      e_hidden_dim: 32
      xi_hidden_dim: 4
      num_layers: 6
      num_bb_update_layers: 6
      dropout: 0.0
    # GCPNet layer config #
    layer_cfg:
      pre_norm: false
      use_gcp_norm: true
      use_gcp_dropout: true
      use_scalar_message_attention: true
      num_feedforward_layers: 2
      dropout: 0.0
      nonlinearity_slope: 1e-2
      # GCPNet message-passing config #
      mp_cfg:
        edge_encoder: false
        edge_gate: false
        num_message_layers: 4
        message_residual: 0
        message_ff_multiplier: 1
        self_message: true

  vqvae:
    vector_quantization:
      dim: 256
      decay: 0.8
      codebook_size: 512
      commitment_weight : 1.0
      alpha: 1.0
    encoder:
      dimension: 128
      num_blocks: 8
    decoder:
      dimension: 128
      num_blocks: 8
      chi_init_dimension: 2
      xi_init_dimension: 1
      activation_function: gelu

train_settings:
  data_path: /bml/acmwhb/Repositories/Lab_Repositories/vq_encoder_decoder/data/validation/validation_set_1024_h5
  num_epochs: 1024
  shuffle: True
  loss: crossentropy
  sample_weight: True
  mixed_precision: fp16 # no, fp16, bf16, fp8
  batch_size: 1
  num_workers: 2
  grad_accumulation: 1
  max_task_samples: 1

valid_settings:
  data_path: /bml/acmwhb/Repositories/Lab_Repositories/vq_encoder_decoder/data/validation/validation_set_1024_h5
  do_every: 2048
  batch_size: 8
  num_workers: 0

visualization_settings:
  data_path: /bml/acmwhb/Repositories/Lab_Repositories/vq_encoder_decoder/data/validation/validation_set_1024_h5
  fasta_path: /bml/acmwhb/Repositories/Lab_Repositories/vq_encoder_decoder/visualization/Rep_subfamily_basedon_S40pdb.fa
  do_every: 4096
  batch_size: 1
  num_workers: 4

optimizer:
  name: schedulerfree
  lr: 8e-5
  weight_decouple: True
  weight_decay: 1e-2
  eps: 1e-16
  beta_1: 0.9
  beta_2: 0.999
  use_8bit_adam: False
  grad_clip_norm: 1
  decay:
    warmup: 128
    min_lr: 1e-7
    gamma: 0.2
    num_restarts: 1